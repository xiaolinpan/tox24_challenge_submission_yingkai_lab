#Hyperparameters for the LSTM, both for the next-token classifier and the regressor

# N epochs with early stopping of LSTM models
epochs: 100
early_stopping_patience: 10
# Validation split for all deep learning methods
val_split: 0.1
# Augment smiles n times
augmentation: 10
#  batch size
batch_size: 32
# list of dropout applied to each LSTM layer
dropout_1: 0.4
dropout_2: 0.4
# list with number of neurons per LSTM layer
layer_1: 1024
layer_2: 256
# starting learning rate
lr: 0.0001
# decrease the learning rate by this factor after 'patience_lr' epochs of no improvement
lr_factor: 0.5
# this is the max length of a smiles including the start/end character
max_len_model: 202
# minimal learning rate
min_lr: 1.0e-05
# what metric to monitor while training
monitor: val_loss
# change the learning rate after n epochs
patience_lr: 5
# save a model every 'period' epochs
period: 2
#  is this LSTM layer trainable?
train_layer1: true
train_layer2: true